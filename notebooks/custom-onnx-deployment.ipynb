{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cb477f3",
   "metadata": {},
   "source": [
    "This notebook deploys the ONNX model we obtained in the `onnx-conversion.ipynb` notebook to Vertex AI using [custom prediction routes (CPR)](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/prediction/custom_prediction_routines/SDK_Pytorch_Custom_Predict.ipynb). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead52da0",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc8ada1b-228d-43c2-91de-82bd2654c5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[GCP-PROJECT]\"\n",
    "PREDICTION_IMAGE_URI = f\"gcr.io/{PROJECT_ID}/resnetv2\"\n",
    "BUCKET_NAME = \"gs://[BUCKET-NAME]\"\n",
    "REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05621d86",
   "metadata": {},
   "source": [
    "## Copy over the initial artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fe2a095-671f-454d-9d1f-3cfcf6609839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0\n"
     ]
    }
   ],
   "source": [
    "LOCAL_FOLDER = \"onnx_deployment_files\"\n",
    "!mkdir -p {LOCAL_FOLDER}\n",
    "!ls -lh {LOCAL_FOLDER}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9062437-0b06-4256-96cf-a8be012ca97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ilsvrc2012_wordnet_lemmas.txt {LOCAL_FOLDER}\n",
    "!cp resnetv2101.onnx {LOCAL_FOLDER}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b189a5f8-a9f9-4fd0-b3ed-1362ce6c19d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "\n",
    "google-cloud-storage>=1.26.0,<2.0.0dev\n",
    "google-cloud-aiplatform[prediction]>=1.16.0\n",
    "onnxruntime==1.11.1\n",
    "numpy==1.22.2\n",
    "tensorflow>=2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c145e14c",
   "metadata": {},
   "source": [
    "## The predictor class\n",
    "\n",
    "This is the meat of our deployment. We define all the logic needed to handle the request payload, run the ONNX model on it, and postprocess the predictions. \n",
    "\n",
    "Refer [here](https://github.com/googleapis/python-aiplatform/tree/custom-prediction-routine/google/cloud/aiplatform/prediction) to know more about this class and how Vertex AI's custom prediction routes are configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c60fbf37-cc79-4277-bafc-d133cd4a37ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing onnx_deployment_files/predictor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {LOCAL_FOLDER}/predictor.py\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "import tensorflow as tf\n",
    "\n",
    "from google.cloud.aiplatform.prediction.predictor import Predictor\n",
    "from google.cloud.aiplatform.utils import prediction_utils\n",
    "\n",
    "\n",
    "IMG_SIZE = 224\n",
    "\n",
    "\n",
    "class ImgClassificationPredictor(Predictor):\n",
    "    def __init__(self):\n",
    "        self._onnx_path = \"resnetv2101.onnx\"\n",
    "        self._labels_path = \"ilsvrc2012_wordnet_lemmas.txt\"\n",
    "\n",
    "    def load(self, artifacts_uri: str):\n",
    "        \"\"\"Loads the model artifacts.\"\"\"\n",
    "        prediction_utils.download_model_artifacts(artifacts_uri)\n",
    "\n",
    "        sess_options = ort.SessionOptions()\n",
    "        sess_options.intra_op_num_threads = os.cpu_count()\n",
    "        self._model_session = ort.InferenceSession(\n",
    "            self._onnx_path, sess_options, providers=[\"CPUExecutionProvider\"]\n",
    "        )\n",
    "\n",
    "        with open(self._labels_path, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "        self._imagenet_int_to_str = [line.rstrip() for line in lines]\n",
    "\n",
    "    def preprocess_bytes(self, bytes_input) -> np.ndarray:\n",
    "        bytes_input = tf.io.decode_base64(bytes_input)\n",
    "        decoded = tf.io.decode_jpeg(bytes_input, channels=3)\n",
    "        decoded = tf.image.convert_image_dtype(decoded, tf.float32)\n",
    "        resized = tf.image.resize(decoded, size=(IMG_SIZE, IMG_SIZE))\n",
    "        return resized\n",
    "\n",
    "    def preprocess(self, prediction_input: Dict) -> np.ndarray:\n",
    "        instances = prediction_input[\"instances\"]\n",
    "        decoded_images = tf.map_fn(\n",
    "            self.preprocess_bytes,\n",
    "            tf.constant(instances),\n",
    "            dtype=tf.float32,\n",
    "            back_prop=False,\n",
    "        )\n",
    "        return decoded_images.numpy()\n",
    "\n",
    "    def predict(self, images: np.ndarray) -> List[str]:\n",
    "        \"\"\"Performs prediction.\"\"\"\n",
    "        predicted_labels = []\n",
    "        logits = self._model_session.run(None, {\"args_0\": images})[0]\n",
    "\n",
    "        for logit in logits:\n",
    "            predicted_labels.append(self._imagenet_int_to_str[int(np.argmax(logits))])\n",
    "\n",
    "        return predicted_labels\n",
    "\n",
    "    def postprocess(self, prediction_results: Tuple) -> Dict[str, List[str]]:\n",
    "        return {\"predictions\": prediction_results}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6195ad35",
   "metadata": {},
   "source": [
    "## Copy over the new artifacts to our GCS bucket for remote predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a437b3f9-eb0d-4db8-807a-838e545c68b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 171M\n",
      "-rw-r--r-- 1 jupyter jupyter  22K Sep 20 03:59 ilsvrc2012_wordnet_lemmas.txt\n",
      "-rw-r--r-- 1 jupyter jupyter 2.1K Sep 20 03:59 predictor.py\n",
      "-rw-r--r-- 1 jupyter jupyter  134 Sep 20 03:59 requirements.txt\n",
      "-rw-r--r-- 1 jupyter jupyter 171M Sep 20 03:59 resnetv2101.onnx\n"
     ]
    }
   ],
   "source": [
    "!cp requirements.txt $LOCAL_FOLDER/requirements.txt\n",
    "!ls -lh $LOCAL_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b93194a-a7f7-4d8c-b2af-43e3cdeae3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://onnx_deployment_files/ilsvrc2012_wordnet_lemmas.txt [Content-Type=text/plain]...\n",
      "/ [1 files][ 21.2 KiB/ 21.2 KiB]                                                \n",
      "Operation completed over 1 objects/21.2 KiB.                                     \n",
      "Copying file://onnx_deployment_files/resnetv2101.onnx [Content-Type=application/octet-stream]...\n",
      "==> NOTE: You are uploading one or more large file(s), which would run          \n",
      "significantly faster if you enable parallel composite uploads. This\n",
      "feature can be enabled by editing the\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\n",
      "configuration file. However, note that if you do this large files will\n",
      "be uploaded as `composite objects\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
      "means that any user who downloads such objects will need to have a\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
      "without a compiled crcmod, computing checksums on composite objects is\n",
      "so slow that gsutil disables downloads of composite objects.\n",
      "\n",
      "\\ [1 files][170.5 MiB/170.5 MiB]                                                \n",
      "Operation completed over 1 objects/170.5 MiB.                                    \n",
      "gs://ccd-ahm-2022/onnx_deployment_files/ilsvrc2012_wordnet_lemmas.txt\n",
      "gs://ccd-ahm-2022/onnx_deployment_files/resnetv2101.onnx\n"
     ]
    }
   ],
   "source": [
    "!gsutil cp {LOCAL_FOLDER}/ilsvrc2012_wordnet_lemmas.txt {BUCKET_NAME}/{LOCAL_FOLDER}/\n",
    "!gsutil cp {LOCAL_FOLDER}/resnetv2101.onnx {BUCKET_NAME}/{LOCAL_FOLDER}/\n",
    "!gsutil ls {BUCKET_NAME}/{LOCAL_FOLDER}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5260bcf-a1cc-4942-a7a7-abe9e25f2344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8.0K\n",
      "-rw-r--r-- 1 jupyter jupyter 2.1K Sep 20 03:59 predictor.py\n",
      "-rw-r--r-- 1 jupyter jupyter  134 Sep 20 03:59 requirements.txt\n"
     ]
    }
   ],
   "source": [
    "# Remove the local artifacts.\n",
    "!rm -rf {LOCAL_FOLDER}/resnetv2101.onnx\n",
    "!rm -rf {LOCAL_FOLDER}/ilsvrc2012_wordnet_lemmas.txt \n",
    "!ls -lh {LOCAL_FOLDER}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9facb9a",
   "metadata": {},
   "source": [
    "## Build the Docker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19fdb3c8-7778-4069-9887-b887a6b227d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from google.cloud.aiplatform.prediction import LocalModel\n",
    "from onnx_deployment_files.predictor import (\n",
    "    ImgClassificationPredictor,\n",
    ")  # Update this path as the variable $USER_SRC_DIR to import the custom predictor.\n",
    "\n",
    "local_model = LocalModel.build_cpr_model(\n",
    "    LOCAL_FOLDER,\n",
    "    PREDICTION_IMAGE_URI,\n",
    "    base_image=\"python:3.8\",\n",
    "    predictor=ImgClassificationPredictor,\n",
    "    requirements_path=os.path.join(LOCAL_FOLDER, \"requirements.txt\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd794757-20d1-49cb-a2bb-5d378137f3b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "image_uri: \"gcr.io/fast-ai-exploration/resnetv2\"\n",
       "predict_route: \"/predict\"\n",
       "health_route: \"/health\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_model.get_serving_container_spec()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d403afe4",
   "metadata": {},
   "source": [
    "## Copy over the model artifacts to a local directory for local predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6ee6e69-3189-446c-a6e4-ff4421334965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://ccd-ahm-2022/onnx_deployment_files/resnetv2101.onnx...\n",
      "| [1 files][170.5 MiB/170.5 MiB]                                                \n",
      "Operation completed over 1 objects/170.5 MiB.                                    \n",
      "Copying gs://ccd-ahm-2022/onnx_deployment_files/ilsvrc2012_wordnet_lemmas.txt...\n",
      "/ [1 files][ 21.2 KiB/ 21.2 KiB]                                                \n",
      "Operation completed over 1 objects/21.2 KiB.                                     \n",
      "ilsvrc2012_wordnet_lemmas.txt  resnetv2101.onnx\n"
     ]
    }
   ],
   "source": [
    "LOCAL_MODEL_ARTIFACTS_DIR = \"model_artifacts\"\n",
    "!mkdir -p {LOCAL_MODEL_ARTIFACTS_DIR}\n",
    "\n",
    "!gsutil cp {BUCKET_NAME}/{LOCAL_FOLDER}/resnetv2101.onnx $LOCAL_MODEL_ARTIFACTS_DIR\n",
    "!gsutil cp {BUCKET_NAME}/{LOCAL_FOLDER}/ilsvrc2012_wordnet_lemmas.txt $LOCAL_MODEL_ARTIFACTS_DIR\n",
    "\n",
    "!ls {LOCAL_MODEL_ARTIFACTS_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9e9c3c",
   "metadata": {},
   "source": [
    "## Healthness check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce4268d5-b717-4170-af1f-f045fb732641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]> b'{}'\n"
     ]
    }
   ],
   "source": [
    "with local_model.deploy_to_local_endpoint(\n",
    "    artifact_uri=f\"{LOCAL_MODEL_ARTIFACTS_DIR}\",\n",
    ") as local_endpoint:\n",
    "    health_check_response = local_endpoint.run_health_check()\n",
    "\n",
    "print(health_check_response, health_check_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea68ee8",
   "metadata": {},
   "source": [
    "## Test the Docker image if it's running ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45ecaf74-e807-49d9-bcbc-0d6e102608c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create request payload\n",
    "\n",
    "import base64\n",
    "import json\n",
    "\n",
    "with open(\"test.jpg\", \"rb\") as f:\n",
    "    data = f.read()\n",
    "b64str = base64.urlsafe_b64encode(data).decode(\"utf-8\")\n",
    "\n",
    "instances = {\"instances\": [b64str]}\n",
    "s = json.dumps(instances)\n",
    "with open(\"instances.json\", \"w\") as f:\n",
    "    f.write(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee62ddb0",
   "metadata": {},
   "source": [
    "### Test with a request body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ce77cc3-3f0f-4c90-a56f-eb1b150e4697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]> b'{\"predictions\": [\"yellow_lady\\'s_slipper, yellow_lady-slipper, Cypripedium_calceolus, Cypripedium_parviflorum\"]}'\n",
      "<Response [200]> b'{}'\n"
     ]
    }
   ],
   "source": [
    "with local_model.deploy_to_local_endpoint(\n",
    "    artifact_uri=f\"{LOCAL_MODEL_ARTIFACTS_DIR}\",\n",
    ") as local_endpoint:\n",
    "    predict_response = local_endpoint.predict(\n",
    "        request=s,\n",
    "        headers={\"Content-Type\": \"application/json\"},\n",
    "    )\n",
    "\n",
    "    health_check_response = local_endpoint.run_health_check()\n",
    "    \n",
    "print(predict_response, predict_response.content)\n",
    "print(health_check_response, health_check_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12956e8f",
   "metadata": {},
   "source": [
    "### Test with a request file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22e30758-6b5e-4eae-8f65-412e454e26a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]> b'{\"predictions\": [\"yellow_lady\\'s_slipper, yellow_lady-slipper, Cypripedium_calceolus, Cypripedium_parviflorum\"]}'\n",
      "<Response [200]> b'{}'\n"
     ]
    }
   ],
   "source": [
    "with local_model.deploy_to_local_endpoint(\n",
    "    artifact_uri=f\"{LOCAL_MODEL_ARTIFACTS_DIR}\",\n",
    ") as local_endpoint:\n",
    "    predict_response = local_endpoint.predict(\n",
    "        request_file=\"instances.json\",\n",
    "        headers={\"Content-Type\": \"application/json\"},\n",
    "    )\n",
    "\n",
    "    health_check_response = local_endpoint.run_health_check()\n",
    "    \n",
    "print(predict_response, predict_response.content)\n",
    "print(health_check_response, health_check_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f836a52e",
   "metadata": {},
   "source": [
    "## Run predictions with the remote artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "098aadde-7aab-43dc-8a9a-1280ea66ec14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]> b'{\"predictions\": [\"yellow_lady\\'s_slipper, yellow_lady-slipper, Cypripedium_calceolus, Cypripedium_parviflorum\"]}'\n",
      "<Response [200]> b'{}'\n"
     ]
    }
   ],
   "source": [
    "with local_model.deploy_to_local_endpoint(\n",
    "    artifact_uri=f\"{BUCKET_NAME}/{LOCAL_FOLDER}\",\n",
    ") as local_endpoint:\n",
    "    predict_response = local_endpoint.predict(\n",
    "        request=s,\n",
    "        headers={\"Content-Type\": \"application/json\"},\n",
    "    )\n",
    "\n",
    "    health_check_response = local_endpoint.run_health_check()\n",
    "    \n",
    "print(predict_response, predict_response.content)\n",
    "print(health_check_response, health_check_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303ec492",
   "metadata": {},
   "source": [
    "## Push the Docker image to GCI for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6593ed7b-b60c-4280-a05b-9a9eede9e859",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model.push_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee7be2c",
   "metadata": {},
   "source": [
    "## Deploy the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a65548e8-92fb-4930-99a2-fb66af8abbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa84044a-514b-4e1d-84df-25e0097942be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model\n",
      "Create Model backing LRO: projects/29880397572/locations/us-central1/models/7113184922780565504/operations/2616277266774097920\n",
      "Model created. Resource name: projects/29880397572/locations/us-central1/models/7113184922780565504@1\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/29880397572/locations/us-central1/models/7113184922780565504@1')\n"
     ]
    }
   ],
   "source": [
    "# Upload the model to Vertex AI (Model Registry)\n",
    "MODEL_DISPLAY_NAME = \"resnetv2101-onnx\"\n",
    "\n",
    "model = aiplatform.Model.upload(\n",
    "    local_model=local_model,\n",
    "    display_name=MODEL_DISPLAY_NAME,\n",
    "    artifact_uri=f\"{BUCKET_NAME}/{LOCAL_FOLDER}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "702c18e3-c4d5-435f-aae5-23299f0a4feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/29880397572/locations/us-central1/endpoints/5357585910617604096/operations/3160086921779085312\n",
      "Endpoint created. Resource name: projects/29880397572/locations/us-central1/endpoints/5357585910617604096\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/29880397572/locations/us-central1/endpoints/5357585910617604096')\n",
      "Deploying model to Endpoint : projects/29880397572/locations/us-central1/endpoints/5357585910617604096\n",
      "Deploy Endpoint model backing LRO: projects/29880397572/locations/us-central1/endpoints/5357585910617604096/operations/2891841268973830144\n",
      "Endpoint model deployed. Resource name: projects/29880397572/locations/us-central1/endpoints/5357585910617604096\n"
     ]
    }
   ],
   "source": [
    "# Deploy the model to an endpoint\n",
    "endpoint = model.deploy(\n",
    "    machine_type=\"n1-standard-8\",\n",
    "    min_replica_count=1,\n",
    "    max_replica_count=3,\n",
    "    autoscaling_target_cpu_utilization=60\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1694645",
   "metadata": {},
   "source": [
    "## Test the Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a6abd902-b6b9-4b55-8ae3-6fe13078f23e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(predictions=[\"yellow_lady's_slipper, yellow_lady-slipper, Cypripedium_calceolus, Cypripedium_parviflorum\"], deployed_model_id='1939454948513153024', model_version_id='1', model_resource_name='projects/29880397572/locations/us-central1/models/7113184922780565504', explanations=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = endpoint.predict(instances=[b64str])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "86be5873-0a2d-4c6c-ab00-6add82337535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"predictions\": [\n",
      "    \"yellow_lady's_slipper, yellow_lady-slipper, Cypripedium_calceolus, Cypripedium_parviflorum\"\n",
      "  ],\n",
      "  \"deployedModelId\": \"1939454948513153024\",\n",
      "  \"model\": \"projects/29880397572/locations/us-central1/models/7113184922780565504\",\n",
      "  \"modelDisplayName\": \"resnetv2101-onnx\",\n",
      "  \"modelVersionId\": \"1\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "ENDPOINT_ID = endpoint.name\n",
    "\n",
    "! curl \\\n",
    "    -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    -d @instances.json \\\n",
    "    https://{REGION}-aiplatform.googleapis.com/v1/projects/{PROJECT_ID}/locations/{REGION}/endpoints/{ENDPOINT_ID}:predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2205a5f",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1c346484-817c-4dc6-90b5-137e917a2099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Undeploying Endpoint model: projects/29880397572/locations/us-central1/endpoints/5357585910617604096\n",
      "Undeploy Endpoint model backing LRO: projects/29880397572/locations/us-central1/endpoints/5357585910617604096/operations/4624882700581339136\n",
      "Endpoint model undeployed. Resource name: projects/29880397572/locations/us-central1/endpoints/5357585910617604096\n",
      "Deleting Endpoint : projects/29880397572/locations/us-central1/endpoints/5357585910617604096\n",
      "Delete Endpoint  backing LRO: projects/29880397572/locations/us-central1/operations/5177699554841067520\n",
      "Endpoint deleted. . Resource name: projects/29880397572/locations/us-central1/endpoints/5357585910617604096\n",
      "Deleting Model : projects/29880397572/locations/us-central1/models/7113184922780565504\n",
      "Delete Model  backing LRO: projects/29880397572/locations/us-central1/operations/5050472865367851008\n",
      "Model deleted. . Resource name: projects/29880397572/locations/us-central1/models/7113184922780565504\n",
      "\u001b[1;33mWARNING:\u001b[0m Implicit \":latest\" tag specified: gcr.io/fast-ai-exploration/resnetv2\n",
      "\u001b[1;33mWARNING:\u001b[0m Successfully resolved tag to sha256, but it is recommended to use sha256 directly.\n",
      "Digests:\n",
      "- gcr.io/fast-ai-exploration/resnetv2@sha256:43fd9f5d69ac24ab741aa4d50d32d933a560f5636fb29ba8ed23887b0c66d51c\n",
      "  Associated tags:\n",
      " - latest\n",
      "Tags:\n",
      "- gcr.io/fast-ai-exploration/resnetv2:latest\n",
      "Deleted [gcr.io/fast-ai-exploration/resnetv2:latest].\n",
      "Deleted [gcr.io/fast-ai-exploration/resnetv2@sha256:43fd9f5d69ac24ab741aa4d50d32d933a560f5636fb29ba8ed23887b0c66d51c].\n",
      "\n",
      "\n",
      "To take a quick anonymous survey, run:\n",
      "  $ gcloud survey\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Undeploy model and delete endpoint\n",
    "endpoint.delete(force=True)\n",
    "\n",
    "# Delete the model resource\n",
    "model.delete()\n",
    "\n",
    "!gcloud container images delete $PREDICTION_IMAGE_URI --quiet\n",
    "\n",
    "# !gsutil rm -r $BUCKET_NAME"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-9.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-9:m94"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
